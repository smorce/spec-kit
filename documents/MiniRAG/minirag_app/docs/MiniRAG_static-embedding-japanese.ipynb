{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6875c5de-f21a-4610-a8fc-ef219ee61dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import os\n",
    "import tempfile\n",
    "from minirag import MiniRAG, QueryParam\n",
    "from minirag.llm.hf import (\n",
    "    hf_model_complete,\n",
    "    hf_embed,\n",
    ")\n",
    "# from minirag.llm.openai import openrouter_openai_complete\n",
    "from minirag.llm.openai import openai_complete_if_cache\n",
    "from minirag.utils import EmbeddingFunc\n",
    "from minirag.utils import (\n",
    "    wrap_embedding_func_with_attrs,\n",
    "    locate_json_string_body_from_string,\n",
    "    safe_unicode_decode,\n",
    "    logger,\n",
    ")\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import asyncio\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941081d-405c-4833-86fa-f5bcdd0b3c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openrouter APIキーが設定されました\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENROUTER_API_KEY\"] = \"*********************\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"*********************\"\n",
    "print(\"Openrouter APIキーが設定されました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16a765-6d23-439b-a433-e40df1cb977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作業ディレクトリ: /tmp/minirag_demo\n"
     ]
    }
   ],
   "source": [
    "# 埋め込みモデルの設定\n",
    "# EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# MINI モードでの回答ができなくなったので、逆に精度が落ちたかも。\n",
    "EMBEDDING_MODEL = \"hotchpotch/static-embedding-japanese\"\n",
    "EMBEDDING_DIM = 1024\n",
    "\n",
    "TOKENIZER_MODEL = \"hotchpotch/xlm-roberta-japanese-tokenizer\"\n",
    "\n",
    "# LLMの設定\n",
    "# LLM_MODEL = \"Qwen/Qwen3-1.7B\"  # または \"Qwen/Qwen3-4B\", \"Qwen/Qwen3-1.7B\" など\n",
    "# LLM_MODEL = \"jaeyong2/Qwen2.5-3B-Instruct-Ja-SFT\"\n",
    "LLM_MODEL = \"deepseek/deepseek-chat-v3-0324:free\"\n",
    "\n",
    "\n",
    "\n",
    "# 作業ディレクトリの作成\n",
    "WORKING_DIR = \"/tmp/minirag_demo\"\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"作業ディレクトリ: {WORKING_DIR}\")\n",
    "\n",
    "\n",
    "# DATA_PATH = args.datapath\n",
    "# QUERY_PATH = args.querypath\n",
    "# OUTPUT_PATH = args.outputpath\n",
    "# print(\"USING LLM:\", LLM_MODEL)\n",
    "# print(\"USING WORKING DIR:\", WORKING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f93e21-c206-4407-8f94-8e670dc30fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1024)\n",
      "0.1040: 素敵なカフェが近所にあるよ。落ち着いた雰囲気でゆっくりできるし、窓際の席からは公園の景色も見えるんだ。\n",
      "0.2521: 新鮮な魚介を提供する店です。地元の漁師から直接仕入れているので鮮度は抜群ですし、料理人の腕も確かです。\n",
      "0.4835: あそこは行きにくいけど、隠れた豚骨の名店だよ。スープが最高だし、麺の硬さも好み。\n",
      "0.3199: おすすめの中華そばの店を教えてあげる。とりわけチャーシューが手作りで柔らかくてジューシーなんだ。\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "HF_TOKEN = \"**********************\"\n",
    "model = SentenceTransformer(EMBEDDING_MODEL, device=\"cpu\", token=HF_TOKEN)\n",
    "\n",
    "query = \"美味しいラーメン屋に行きたい\"\n",
    "docs = [\n",
    "    \"素敵なカフェが近所にあるよ。落ち着いた雰囲気でゆっくりできるし、窓際の席からは公園の景色も見えるんだ。\",\n",
    "    \"新鮮な魚介を提供する店です。地元の漁師から直接仕入れているので鮮度は抜群ですし、料理人の腕も確かです。\",\n",
    "    \"あそこは行きにくいけど、隠れた豚骨の名店だよ。スープが最高だし、麺の硬さも好み。\",\n",
    "    \"おすすめの中華そばの店を教えてあげる。とりわけチャーシューが手作りで柔らかくてジューシーなんだ。\",\n",
    "]\n",
    "\n",
    "embeddings = model.encode([query] + docs)\n",
    "print(embeddings.shape)\n",
    "similarities = model.similarity(embeddings[0], embeddings[1:])\n",
    "for i, similarity in enumerate(similarities[0].tolist()):\n",
    "    print(f\"{similarity:.04f}: {docs[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00688add-fcf5-4517-85c1-2eae3ae5f696",
   "metadata": {},
   "source": [
    "## transformer の API で使えるように変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87149d98-7010-4b7d-96c1-b0b2566fa72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n",
    "\n",
    "\n",
    "class StaticEmbeddingConfig(PretrainedConfig):\n",
    "    model_type = \"static-embedding\"\n",
    "\n",
    "    def __init__(self, vocab_size=32768, hidden_size=1024, pad_token_id=0, **kwargs):\n",
    "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "class StaticEmbeddingModel(PreTrainedModel):\n",
    "    config_class = StaticEmbeddingConfig\n",
    "\n",
    "    def __init__(self, config: StaticEmbeddingConfig):\n",
    "        super().__init__(config)\n",
    "        # ★ EmbeddingBag そのものでも OK ですが、\n",
    "        #   シーケンス長をそろえて attention_mask で平均を取る方が扱いやすいので nn.Embedding に変更\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.hidden_size,\n",
    "            padding_idx=config.pad_token_id,\n",
    "        )\n",
    "        self.post_init()  # transformers の重み初期化\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        - input_ids      : (batch, seq_len)\n",
    "        - attention_mask : (batch, seq_len) — 0 は padding\n",
    "        戻り値は Transformers 共通の BaseModelOutputWithPoolingAndCrossAttentions\n",
    "        \"\"\"\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != self.config.pad_token_id).int()\n",
    "\n",
    "        token_embs = self.embedding(input_ids)                       # (B, L, H)\n",
    "        # マスク付き平均プール\n",
    "        masked_embs = token_embs * attention_mask.unsqueeze(-1)      # (B, L, H)\n",
    "        lengths = attention_mask.sum(dim=1, keepdim=True).clamp(min=1e-8)  # (B, 1)\n",
    "        sentence_emb = masked_embs.sum(dim=1) / lengths              # (B, H)\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=token_embs,  # ここでは token レベルをそのまま\n",
    "            pooler_output=sentence_emb,    # 文ベクトル\n",
    "            attentions=None,\n",
    "            cross_attentions=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17442fc4-fc20-4821-8812-596ff97f4496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 変換完了 — 保存先: ./static-embedding-transformers\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SentenceTransformer 版 (hotchpotch/static-embedding-japanese) から\n",
    "StaticEmbeddingModel へ重みをコピーして保存するスクリプト\n",
    "\"\"\"\n",
    "\n",
    "SRC = \"hotchpotch/static-embedding-japanese\"   # オリジナル\n",
    "DST = \"./static-embedding-transformers\"        # 保存先\n",
    "\n",
    "# ① SentenceTransformer を読み込む\n",
    "st_model = SentenceTransformer(SRC)\n",
    "embedding_weight = st_model[0].embedding.weight.data   # nn.EmbeddingBag の重みを取得\n",
    "\n",
    "# ② Config → Model を作成\n",
    "config = StaticEmbeddingConfig(\n",
    "    vocab_size=embedding_weight.size(0),\n",
    "    hidden_size=embedding_weight.size(1),\n",
    "    pad_token_id=0,           # トークナイザの <pad> が id=0\n",
    ")\n",
    "model = StaticEmbeddingModel(config)\n",
    "\n",
    "# ③ 重みコピー\n",
    "with torch.no_grad():\n",
    "    model.embedding.weight.copy_(embedding_weight)\n",
    "\n",
    "# ④ save_pretrained で書き出し\n",
    "model.save_pretrained(DST)\n",
    "# st_model.tokenizer.save_pretrained(DST)   # tokenizer.json なども一緒に保存\n",
    "\n",
    "print(f\"✅ 変換完了 — 保存先: {DST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd90d1ce-899e-4e4b-8c4b-ba6bc9545e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([2, 1024])\n",
      "cosine: 0.4834601879119873\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_DIR = \"./static-embedding-transformers\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL)\n",
    "model     = StaticEmbeddingModel.from_pretrained(MODEL_DIR)\n",
    "\n",
    "sentences = [\n",
    "    \"美味しいラーメン屋に行きたい\",\n",
    "    \"あそこは行きにくいけど、隠れた豚骨の名店だよ。スープが最高だし、麺の硬さも好み。\",\n",
    "]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    sentences,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=False,   # 元モデルは special tokens なし\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    vecs = outputs.pooler_output     # (batch, hidden_size)\n",
    "\n",
    "print(\"shape:\", vecs.shape)          # torch.Size([2, 1024])\n",
    "similarity = torch.nn.functional.cosine_similarity(vecs[0], vecs[1], dim=0)\n",
    "print(\"cosine:\", similarity.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e501a8-78d3-431e-8849-cc7865683db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def openrouter_openai_complete(\n",
    "    prompt,\n",
    "    system_prompt=None,\n",
    "    history_messages=[],\n",
    "    keyword_extraction=False,\n",
    "    api_key: str = None,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "    # if api_key:\n",
    "    #     os.environ[\"OPENROUTER_API_KEY\"] = api_key\n",
    "\n",
    "    keyword_extraction = kwargs.pop(\"keyword_extraction\", None)\n",
    "    result = await openai_complete_if_cache(\n",
    "        LLM_MODEL,  # change accordingly\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=api_key,\n",
    "        **kwargs,\n",
    "    )\n",
    "    if keyword_extraction:  # TODO: use JSON API\n",
    "        return locate_json_string_body_from_string(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ea62583-fcbc-4e2a-9513-cc6bd4860f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app\n",
      "MiniRAG_static-embedding-japanese.ipynb  setup.py\n",
      "minirag\t\t\t\t\t static-embedding-transformers\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31b8ac6d-2322-4339-ae19-9b5c6193e521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-vectordb:Load (31, 1024) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/tmp/minirag_demo/vdb_entities.json'} 31 data\n",
      "INFO:nano-vectordb:Load (31, 1024) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/tmp/minirag_demo/vdb_entities_name.json'} 31 data\n",
      "INFO:nano-vectordb:Load (39, 1024) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/tmp/minirag_demo/vdb_relationships.json'} 39 data\n",
      "INFO:nano-vectordb:Load (4, 1024) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/tmp/minirag_demo/vdb_chunks.json'} 4 data\n",
      "INFO:minirag:Loaded document status storage with 4 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniRAGが初期化されました！\n"
     ]
    }
   ],
   "source": [
    "# MiniRAGインスタンスの作成\n",
    "rag = MiniRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "\n",
    "    # ポスグレ\n",
    "    # kv_storage=\"PGKVStorage\",\n",
    "    # vector_storage=\"PGVectorStorage\",\n",
    "    # graph_storage=\"PGGraphStorage\",\n",
    "\n",
    "    # llm_model_func=hf_model_complete,\n",
    "    # llm_model_func=gemini_2_5_flash_complete,\n",
    "    llm_model_func=openrouter_openai_complete,\n",
    "\n",
    "    llm_model_max_token_size=200,\n",
    "    llm_model_name=LLM_MODEL,\n",
    "    embedding_func=EmbeddingFunc(\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        max_token_size=1000,\n",
    "        func=lambda texts: hf_embed(\n",
    "            texts,\n",
    "            tokenizer=tokenizer,\n",
    "            embed_model=model,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"MiniRAGが初期化されました！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac44cacb-928d-4b21-acad-3ee53633b357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:No new unique documents were found.\n",
      "INFO:minirag:No documents to process\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データを挿入中...\n",
      "テキスト 1/4 を挿入中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1 chunks, 5 entities(duplicated), 4 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠼ Processed 4 chunks, 31 entities(duplicated), 22 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 31 vectors to entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 26.10batch/s]\n",
      "INFO:minirag:Inserting 31 vectors to entities\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 103.74batch/s]\n",
      "INFO:minirag:Inserting 31 vectors to entities_name\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 658.45batch/s]\n",
      "INFO:minirag:Inserting 22 vectors to relationships\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 118.00batch/s]\n",
      "INFO:minirag:Writing graph with 40 nodes, 48 edges\n",
      "INFO:minirag:No new unique documents were found.\n",
      "INFO:minirag:No documents to process\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキスト 2/4 を挿入中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠸ Processed 3 chunks, 19 entities(duplicated), 15 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠼ Processed 4 chunks, 27 entities(duplicated), 21 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 27 vectors to entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 40.25batch/s]\n",
      "INFO:minirag:Inserting 27 vectors to entities\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 128.83batch/s]\n",
      "INFO:minirag:Inserting 27 vectors to entities_name\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 438.55batch/s]\n",
      "INFO:minirag:Inserting 21 vectors to relationships\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 130.64batch/s]\n",
      "INFO:minirag:Writing graph with 40 nodes, 55 edges\n",
      "INFO:minirag:No new unique documents were found.\n",
      "INFO:minirag:No documents to process\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキスト 3/4 を挿入中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1 chunks, 2 entities(duplicated), 3 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠼ Processed 4 chunks, 21 entities(duplicated), 22 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 21 vectors to entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 15.74batch/s]\n",
      "INFO:minirag:Inserting 21 vectors to entities\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 97.41batch/s]\n",
      "INFO:minirag:Inserting 21 vectors to entities_name\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 624.34batch/s]\n",
      "INFO:minirag:Inserting 22 vectors to relationships\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 66.00batch/s]\n",
      "INFO:minirag:Writing graph with 40 nodes, 56 edges\n",
      "INFO:minirag:No new unique documents were found.\n",
      "INFO:minirag:No documents to process\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキスト 4/4 を挿入中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.432895 seconds\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.857874 seconds\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1752853680000'}, 'provider_name': None}}, 'user_id': 'user_2eRMIrkhoLsGuVqbOgFuCKdizrH'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# イベントループが既に実行中の場合\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m insert_texts(rag, sample_texts)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# 新しいループで実行\u001b[39;00m\n\u001b[32m     49\u001b[39m     asyncio.run(insert_texts(rag, sample_texts))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36minsert_texts\u001b[39m\u001b[34m(rag_instance, texts)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(texts):\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mテキスト \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m を挿入中...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m rag_instance.ainsert(text.strip())\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mすべてのデータが挿入されました！\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/minirag/minirag.py:380\u001b[39m, in \u001b[36mMiniRAG.ainsert\u001b[39m\u001b[34m(self, input, split_by_character, split_by_character_only, ids)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inserting_chunks:\n\u001b[32m    379\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mPerforming entity extraction on newly processed chunks\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m extract_entities(\n\u001b[32m    381\u001b[39m         inserting_chunks,\n\u001b[32m    382\u001b[39m         knowledge_graph_inst=\u001b[38;5;28mself\u001b[39m.chunk_entity_relation_graph,\n\u001b[32m    383\u001b[39m         entity_vdb=\u001b[38;5;28mself\u001b[39m.entities_vdb,\n\u001b[32m    384\u001b[39m         entity_name_vdb=\u001b[38;5;28mself\u001b[39m.entity_name_vdb,\n\u001b[32m    385\u001b[39m         relationships_vdb=\u001b[38;5;28mself\u001b[39m.relationships_vdb,\n\u001b[32m    386\u001b[39m         global_config=asdict(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m    387\u001b[39m     )\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._insert_done()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/minirag/operate.py:332\u001b[39m, in \u001b[36mextract_entities\u001b[39m\u001b[34m(chunks, knowledge_graph_inst, entity_vdb, entity_name_vdb, relationships_vdb, global_config)\u001b[39m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(maybe_nodes), \u001b[38;5;28mdict\u001b[39m(maybe_edges)\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# use_llm_func is wrapped in ascynio.Semaphore, limiting max_async callings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    333\u001b[39m     *[_process_single_content(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m ordered_chunks]\n\u001b[32m    334\u001b[39m )\n\u001b[32m    335\u001b[39m \u001b[38;5;28mprint\u001b[39m()  \u001b[38;5;66;03m# clear the progress bar\u001b[39;00m\n\u001b[32m    336\u001b[39m maybe_nodes = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/minirag/operate.py:275\u001b[39m, in \u001b[36mextract_entities.<locals>._process_single_content\u001b[39m\u001b[34m(chunk_key_dp)\u001b[39m\n\u001b[32m    273\u001b[39m history = pack_user_ass_to_openai_messages(hint_prompt, final_result)\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m now_glean_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(entity_extract_max_gleaning):\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     glean_result = \u001b[38;5;28;01mawait\u001b[39;00m use_llm_func(continue_prompt, history_messages=history)\n\u001b[32m    277\u001b[39m     history += pack_user_ass_to_openai_messages(continue_prompt, glean_result)\n\u001b[32m    278\u001b[39m     final_result += glean_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/minirag/utils.py:110\u001b[39m, in \u001b[36mlimit_async_func_call.<locals>.final_decro.<locals>.wait_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(waitting_time)\n\u001b[32m    109\u001b[39m __current_size += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    111\u001b[39m __current_size -= \u001b[32m1\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mopenrouter_openai_complete\u001b[39m\u001b[34m(prompt, system_prompt, history_messages, keyword_extraction, api_key, **kwargs)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopenrouter_openai_complete\u001b[39m(\n\u001b[32m      2\u001b[39m     prompt,\n\u001b[32m      3\u001b[39m     system_prompt=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# if api_key:\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m#     os.environ[\"OPENROUTER_API_KEY\"] = api_key\u001b[39;00m\n\u001b[32m     12\u001b[39m     keyword_extraction = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mkeyword_extraction\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m openai_complete_if_cache(\n\u001b[32m     14\u001b[39m         LLM_MODEL,  \u001b[38;5;66;03m# change accordingly\u001b[39;00m\n\u001b[32m     15\u001b[39m         prompt,\n\u001b[32m     16\u001b[39m         system_prompt=system_prompt,\n\u001b[32m     17\u001b[39m         history_messages=history_messages,\n\u001b[32m     18\u001b[39m         base_url=\u001b[33m\"\u001b[39m\u001b[33mhttps://openrouter.ai/api/v1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m         api_key=api_key,\n\u001b[32m     20\u001b[39m         **kwargs,\n\u001b[32m     21\u001b[39m     )\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m keyword_extraction:  \u001b[38;5;66;03m# TODO: use JSON API\u001b[39;00m\n\u001b[32m     23\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m locate_json_string_body_from_string(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/minirag/llm/openai.py:127\u001b[39m, in \u001b[36mopenai_complete_if_cache\u001b[39m\u001b[34m(model, prompt, system_prompt, history_messages, base_url, api_key, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m openai_async_client.beta.chat.completions.parse(\n\u001b[32m    124\u001b[39m         model=model, messages=messages, **kwargs\n\u001b[32m    125\u001b[39m     )\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m openai_async_client.chat.completions.create(\n\u001b[32m    128\u001b[39m         model=model, messages=messages, **kwargs\n\u001b[32m    129\u001b[39m     )\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response, \u001b[33m\"\u001b[39m\u001b[33m__aiter__\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:2454\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2411\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2412\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2413\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2451\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2452\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2453\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2455\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2456\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2457\u001b[39m             {\n\u001b[32m   2458\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2459\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2460\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2461\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2462\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2463\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2464\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2465\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2466\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2467\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2468\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2469\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2470\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2471\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2472\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2473\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2474\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2475\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2476\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2477\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2478\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2479\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2480\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2481\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2482\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2483\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2484\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2485\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2486\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2487\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2488\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2489\u001b[39m             },\n\u001b[32m   2490\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2491\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2492\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2493\u001b[39m         ),\n\u001b[32m   2494\u001b[39m         options=make_request_options(\n\u001b[32m   2495\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2496\u001b[39m         ),\n\u001b[32m   2497\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2498\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2499\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2500\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:1791\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1779\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1786\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1787\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1788\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1789\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1790\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:1591\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1588\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1590\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1591\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1593\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1752853680000'}, 'provider_name': None}}, 'user_id': 'user_2eRMIrkhoLsGuVqbOgFuCKdizrH'}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.437189 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1 chunks, 6 entities(duplicated), 5 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.817176 seconds\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Processed 2 chunks, 16 entities(duplicated), 12 relations(duplicated)"
     ]
    }
   ],
   "source": [
    "# 約12分かかった\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# サンプルテキストデータ\n",
    "sample_texts = [\n",
    "    \"\"\"\n",
    "今日は素晴らしい一日でした。朝早く起きて、近所の公園を散歩しました。\n",
    "桜の花が満開で、とても美しかったです。午後は友人と映画を見に行きました。\n",
    "「君の名は。」という映画で、とても感動的でした。\n",
    "夜は家族と一緒に夕食を取り、楽しい時間を過ごしました。\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "昨日は仕事で大きなプロジェクトが完了しました。\n",
    "チーム全員で3ヶ月間取り組んできたAIシステムの開発が終わりました。\n",
    "機械学習モデルの精度が95%を超え、クライアントからも高い評価をいただきました。\n",
    "今夜はチームメンバーと祝賀会を開く予定です。\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "週末は料理に挑戦しました。初めてパスタを一から作ってみました。\n",
    "小麦粉から麺を作るのは思っていたより難しかったですが、\n",
    "最終的にはとても美味しいカルボナーラができました。\n",
    "次回はリゾットに挑戦してみたいと思います。\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "読書が趣味で、最近は村上春樹の「ノルウェイの森」を読んでいます。\n",
    "主人公の心情描写がとても繊細で、引き込まれます。\n",
    "また、技術書も読んでおり、「深層学習」について学んでいます。\n",
    "理論と実践のバランスが取れた良い本だと思います。\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "# データの挿入\n",
    "print(\"データを挿入中...\")\n",
    "\n",
    "async def insert_texts(rag_instance, texts):\n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"テキスト {i+1}/{len(texts)} を挿入中...\")\n",
    "        await rag_instance.ainsert(text.strip())\n",
    "\n",
    "    print(\"\\nすべてのデータが挿入されました！\")\n",
    "\n",
    "\n",
    "# イベントループが既に実行中の場合\n",
    "try:\n",
    "    await insert_texts(rag, sample_texts)\n",
    "except RuntimeError:\n",
    "    # 新しいループで実行\n",
    "    asyncio.run(insert_texts(rag, sample_texts))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"処理時間: {elapsed_time:.4f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35031a37-d7b7-4a17-875e-a3f165cdb288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Query: 映画について教えて, top_k: 60, cosine_better_than_threshold: 0.2\n",
      "INFO:minirag:Truncate 1 to 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "クエリ: 映画について教えて\n",
      "==================================================\n",
      "\n",
      "--- NAIVEモード ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# 約3分かかった\n",
    "\n",
    "# # サンプルクエリ\n",
    "# queries = [\n",
    "#     \"映画について教えて\",\n",
    "#     \"仕事のプロジェクトはどうでしたか？\",\n",
    "#     \"料理で何を作りましたか？\",\n",
    "#     \"読んでいる本について教えて\",\n",
    "#     \"散歩について詳しく教えて\"\n",
    "# ]\n",
    "\n",
    "# # 各モードでクエリを実行。この3つがある\n",
    "# modes = [\"naive\", \"mini\", \"light\"]\n",
    "\n",
    "# for query in queries:\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"クエリ: {query}\")\n",
    "#     print(f\"{'='*50}\")\n",
    "\n",
    "#     for mode in modes:\n",
    "#         print(f\"\\n--- {mode.upper()}モード ---\")\n",
    "#         try:\n",
    "#             answer = rag.query(query, param=QueryParam(mode=mode))     # .replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "#             print(f\"回答: {answer}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"エラー: {e}\")\n",
    "\n",
    "\n",
    "async def run_queries(rag_instance, queries):\n",
    "    # 各モードでクエリを実行。この3つがある\n",
    "    modes = [\"naive\", \"mini\", \"light\"]\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"クエリ: {query}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        for mode in modes:\n",
    "            print(f\"\\n--- {mode.upper()}モード ---\")\n",
    "            try:\n",
    "                # 非同期でクエリを実行\n",
    "                answer = await rag_instance.aquery(query, param=QueryParam(mode=mode))\n",
    "                print(f\"回答: {answer}\")\n",
    "            except Exception as e:\n",
    "                print(f\"エラー: {e}\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sample_queries = [\n",
    "    \"映画について教えて\",\n",
    "    \"仕事のプロジェクトはどうでしたか？\",\n",
    "    \"料理で何を作りましたか？\",\n",
    "    \"読んでいる本について教えて\",\n",
    "    \"散歩について詳しく教えて\"\n",
    "]\n",
    "\n",
    "# イベントループが既に実行中の場合\n",
    "try:\n",
    "    await run_queries(rag, sample_queries)\n",
    "except RuntimeError:\n",
    "    # 新しいループで実行\n",
    "    asyncio.run(run_queries(rag, sample_queries))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"処理時間: {elapsed_time:.4f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5621b47-01eb-468b-9e18-6d273ab147f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b99ea-ba92-4bc1-9669-1f3efaa14d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
